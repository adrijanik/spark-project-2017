%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LaTeX Example: Project Report
%
% Source: http://www.howtotex.com
%
% Feel free to distribute this example, but please keep the referral
% to howtotex.com
% Date: March 2011 
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% How to use writeLaTeX: 
%
% You edit the source code here on the left, and the preview on the
% right shows you the result within a few seconds.
%
% Bookmark this page and share the URL with your co-authors. They can
% edit at the same time!
%
% You can upload figures, bibliographies, custom classes and
% styles using the files menu.
%
% If you're new to LaTeX, the wikibook is a great place to start:
% http://en.wikibooks.org/wiki/LaTeX
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Edit the title below to update the display in My Documents
%\title{Project Report}
%
%%% Preamble
\documentclass[paper=a4, fontsize=11pt]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage{fourier}

\usepackage[english]{babel}															% English language/hyphenation
\usepackage[protrusion=true,expansion=true]{microtype}	
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage[pdftex]{graphicx}	
\usepackage{url}


%%% Custom sectioning
\usepackage{sectsty}
\allsectionsfont{\centering \normalfont\scshape}

\usepackage{enumitem}

% if you want to create a new list from scratch
\newlist{alphalist}{enumerate}{1}
% in that case, at least label must be specified using \setlist
\setlist[alphalist,1]{label=\textbf{\alph*.}}
%%% Custom headers/footers (fancyhdr package)
\usepackage{fancyhdr}
\pagestyle{fancyplain}
\fancyhead{}											% No page header
\fancyfoot[L]{}											% Empty 
\fancyfoot[C]{}											% Empty
\fancyfoot[R]{\thepage}									% Pagenumbering
\renewcommand{\headrulewidth}{0pt}			% Remove header underlines
\renewcommand{\footrulewidth}{0pt}				% Remove footer underlines
\setlength{\headheight}{13.6pt}


%%% Equation and float numbering
\numberwithin{equation}{section}		% Equationnumbering: section.eq#
\numberwithin{figure}{section}			% Figurenumbering: section.fig#
\numberwithin{table}{section}				% Tablenumbering: section.tab#


%%% Maketitle metadata
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} 	% Horizontal rule

\title{
		%\vspace{-1in} 	
		\usefont{OT1}{bch}{b}{n}
		\normalfont \normalsize \textsc{University of Nice, Sophia Antipolis} \\ [25pt]
		\horrule{0.5pt} \\[0.4cm]
		\huge Project in Spark 2017 \\
		\horrule{2pt} \\[0.5cm]
}
\author{
		\normalfont \normalsize
        Adrianna Janik\\	\normalfont \normalsize
Ion Mosnoi\\	\normalfont \normalsize
Lei Guo \\ \normalsize
        \today
}
\date{}


%%% Begin document
\usepackage{listings}

\usepackage{xcolor}
\definecolor{commentgreen}{RGB}{2,112,10}
\definecolor{eminence}{RGB}{108,48,130}
\definecolor{weborange}{RGB}{255,165,0}
\definecolor{frenchplum}{RGB}{129,20,83}

\usepackage{listings}
\lstset {
    language=scala,
    frame=tb,
    tabsize=4,
    showstringspaces=false,
    numbers=left,
    %upquote=true,
    commentstyle=\color{commentgreen},
    keywordstyle=\color{eminence},
    stringstyle=\color{red},
    basicstyle=\small\ttfamily, % basic font setting
    emph={int,char,double,float,unsigned,void,bool},
    emphstyle={\color{blue}},
    escapechar=\&,
    % keyword highlighting
    classoffset=1, % starting new class
    otherkeywords={>,<,.,;,-,!,=,~},
    morekeywords={>,<,.,;,-,!,=,~},
    keywordstyle=\color{weborange},
    classoffset=0,
    breaklines=true,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}
\begin{document}
\maketitle
\section{Task}
Firstly we uncompressed the data stored in ling-spam.zip folder with \textit{Extract all} command. 
Secondly we open Virtual Box machine with Hortonworks, we signed in with maria\_dev username and maria\_dev password on Ambari available under 127.0.0.1:8080 ip address. We have selected \textit{Files view}, than navigated to \textit{/tmp} folder and created directories \textit{tmp/ling-spam/ham} and \textit{ling-spam/spam}. Following that we logged in with ssh credentials to Hortonworks machine
\begin{lstlisting}[language=bash]
$ssh root@127.0.0.1 -p 2222
\end{lstlisting}
In the meantime upload to the virtual machine ling-spam.zip with:
\begin{lstlisting}[language=bash]
$sudo scp -P 2222 ../ling-spam.zip  root@127.0.0.1:/tmp/
\end{lstlisting}
We unzipped ling-spam.zip with:
\begin{lstlisting}[language=bash]
$unzip ling-spam.zip -d /tmp/ling-spam
\end{lstlisting}
We putted files into /tmp/ling-spam/ folder in hdfs with:
\begin{lstlisting}[language=bash]
$hdfs dfs -put ./ling-spam/ham /tmp/ling-spam/ham
$hdfs dfs -put ./ling-spam/spam /tmp/ling-spam/spam
\end{lstlisting}

\section{Task}
Installation of sbt:
\begin{lstlisting}[language=bash]
$wget http://dl.bintray.com/sbt/rpm/sbt-0.13.12.rpm
\end{lstlisting}
Edit file /etc/yum.repos.d/sandbox.repo:
\begin{lstlisting}[language=bash]
~[sandbox]
~name=Sandbox repository (tutorials)
~gpgcheck=0
~enabled=0
~baseurl=http://dev2.hortonworks.com.s3.amazonaws.com/repo/dev/master/utils/
\end{lstlisting}

\begin{lstlisting}[language=bash]
$yum clean all
$yum update
$sudo yum localinstall sbt-0.13.12.rpm
$sbt -update
$sudo scp -P 2222 -r ../spamTopWords/*  root@127.0.0.1:/tmp/spamTopWords/
$sbt package
\end{lstlisting}



\section{Task}
Firstly we created Spark Context with:
\begin{lstlisting}[language=scala]
val conf = new SparkConf(.setAppName(``Spam Filter Application'').setMaster(``local'')
val sc = new SparkContext(conf))
\end{lstlisting}
Than we called function \textit{probaWordDir} with defined spark context as well as folder name for which we want to count words.
\begin{lstlisting}[language=scala]
val (probaHW, nbHFiles) = probaWordDir(sc)(args(0)+"ham/*.txt")
print("number of files in "+ args(0)+"ham/*.txt" +":")
println(nbHFiles)


//process spam files
val (probaSW, nbSFiles) = probaWordDir(sc)(args(0)+"spam/*.txt")
print("number of files in "+ args(0)+"spam/*.txt" +":")
println(nbSFiles)
\end{lstlisting}

Function: probaWordDir:
\begin{lstlisting}
def probaWordDir(sc:SparkContext)(filesDir:String)
:(RDD[(String, Double)], Long) = {

      //sc -> class org.apache.spark.SparkContext
      //filesDir -> java.lang.String
      //read the files
      val rdd = sc.wholeTextFiles(filesDir)
      //rdd -> class org.apache.spark.rdd.MapPartitionsRDD
      // The number of files is counted and stored in a variable nbFiles
      val nbFiles = rdd.count()
      //nbFiles -> long
      // Non informative words must be removed from the set of unique words.
      val stopWords = Set(".", ":", ",", " ", "/", "\\", "-", "'", "(", ")", "@", "Subject:")
      //stopWords -> class scala.collection.immutable.HashSet\$HashTrieSet
      // get the words in an email, delete the dublicate in one email, delete     the stop words
      val wordBagRdd: RDD[(String, Set[String])] = rdd.map(textTuple =>
              (textTuple._1, textTuple._2.trim().
              split("\\s+").toSet.diff(stopWords)))
      //wordBagRdd -> class org.apache.spark.rdd.MapPartitionsRDD
      // count the words in all emails
      val wordCountRdd: RDD[(String, Int)] = wordBagRdd.flatMap(x => x._2.map(y => (y, 1))).reduceByKey(_ +_)
      //wordCountRdd -> class org.apache.spark.rdd.ShuffledRDD
      //calculate the probability
      val probaWord: RDD[(String, Double)] = wordCountRdd.map(x => (x._1, x._2.toDouble / nbFiles))
      //probaWord -> class org.apache.spark.rdd.MapPartitionsRDD
      return (probaWord, nbFiles)


}
\end{lstlisting}


\section{Task}
We computed function: computeMutualInformationFactor with given formula: \\ \\
$P(occurs, class)log_2 \Big(\frac{P(occurs, class)}{P(occurs)P(class)}\Big)$ \\
\begin{lstlisting}
def computeMutualInformationFactor(
  probaWC: RDD[(String, Double)],//prob of just a class, some word could not be 
  probaW: RDD[(String, Double)],//all words prob, all word
  probaC: Double, //prb of a class : class mails / all mails
  probaDefault: Double // default value when a probability is missing
): RDD[(String, Double)] = {
           //got (word,(prob for both classes, prob for class)), if the prob f    or class does not exist set the default 
          val probWJoin: RDD[(String, (Double, Option[Double]))] = probaW.leftOuterJoin(probaWC)// got all class probs, if not -> default
                              //p(accurs)  p(accurs,class) 
	//probWJoin -> class org.apache.spark.rdd.MapPartitionsRDD
          val valueClassAndOcu: RDD[(String, (Double, Double))] = probWJoin.map(x => (x._1, (x._2._1, x._2._2.getOrElse(probaDefault))))
          //calculate the formula for mutual information
          valueClassAndOcu.map(x => (x._1, x._2._2 * (math.log(x._2._2 / (x._2._1 * probaC)) / math.log(2.0))))
	  //valueClassAndOcu -> class org.apache.spark.rdd.MapPartitionsRDD

}
\end{lstlisting}

probaWC is a RDD with the map structure: word => probability the word occurs in an email of a given class. \\
probaW has the map structure: word => probability the word occurs (whatever the class).\\ 
probaC is the probability that an email belongs to the given class. \\
probaDefault is a probability when a word does not occur in both classes but only one with value given by formula: \\
$\frac{0.2}{totalNumberOfFiles}$\\
This function returns the factor of each words (so it returns a RDD) given a class value (spam or ham) and an occurrence value
(true or false).

\section{Task}

\begin{alphalist}
	\item We computed the couples (probaWordHam, nbFilesHam) for the directory 'ham' and (probaWordSpam, nbFilesSpam) for the directory 'spam'.
	\item We computed the probability P(occurs, class) for each word. There are two values of class ('ham' and 'spam') and two values of occurs ('true' or 'true'). Hence, we obtained 4 RDDs, one RDD for each case: (true,ham), (true, spam), (false, ham) and (false, spam). Each RDD has the map structure: word => probability the word occurs (or not) in an email of a given class.
	\item We computed the mutual information of each word as a RDD with the map structure: word => MI(word). With the usage of the function	computeMutualInformationFactor. If a word occurs in only one class, its	joint probability with the other class takes on the default value probaDefault defined earlier.	The function computeMutualInformationFactor is called 4 times for each possible value of P(occurs, class): (true,ham), (true, spam), (false, ham) and (false, spam).
	\item The main function  prints on screen the 20 top words (maximizing the mutual information value) which can be used to distinguish a spam from an ham email by using the mutual information. \\

We have obtained this list of top 20 words: \\
      \begin{itemize}
\item (bio,23.820986127869574)
\item (touch-tone,23.820986127869574)
\item (woodland,23.820986127869574)
\item (8080,23.820986127869574)
\item (ibi,23.820986127869574)
\item (wales,23.820986127869574)
\item (pearce,23.820986127869574)
\item (slap,23.820986127869574)
\item (commissioner,23.820986127869574)
\item (&n,23.820986127869574)
\item (2442,23.820986127869574)
\item (cake,23.820986127869574)
\item (dawson,23.820986127869574)
\item (detailed,23.820986127869574)
\item (trilogy,23.820986127869574)
\item (miranda,23.820986127869574)
\item (piggy,23.820986127869574)
\item (marke,23.820986127869574)
\item (lightn,23.820986127869574)
\item (pristine,23.820986127869574)
     
      \end{itemize}
	\item These top words are also stored on HDFS in the file
	'/tmp/topWords.txt'
\end{alphalist}
Main function:
\begin{lstlisting}
def main(args: Array[String]) {

      if(args.size > 0){
              val conf = new SparkConf().setAppName("Spam Filter Application").setMaster("local")
	      //conf -> class org.apache.spark.SparkConf
	      //initiate spark context
              val sc = new SparkContext(conf)
	      //sc ->  class org.apache.spark.SparkContext
              println("Got the path:"+args(0))
              // args(0) should be something like "hdfs:///project/, see readme

              //process ham files
              val (probaHW, nbHFiles) = probaWordDir(sc)(args(0)+"ham/*.txt")
              //probaHW -> class org.apache.spark.rdd.MapPartitionsRDD
	      //nbHFiles -> long
              //process spam files
              val (probaSW, nbSFiles) = probaWordDir(sc)(args(0)+"spam/*.txt")
	      //probaSW -> class org.apache.spark.rdd.MapPartitionsRDD
	      // nbSFiles -> long
              print("number of files in "+ args(0)+"ham/*.txt" +":")
              println(nbHFiles)
              print("number of files in "+ args(0)+"spam/*.txt" +":")
              println(nbSFiles)

              val nbFiles = nbSFiles + nbHFiles
	      //nbFiles -> long
             
              val probaWs = probaSW.map(x => (x._1,(x._2,1))).union(probaHW.map(x => (x._1,(x._2,0))))
              //probaWs -> class org.apache.spark.rdd.UnionRDD
              val probaW = probaWs.reduceByKey((x,y) => if(y._2<1) ((x._1*nbSFiles.toDouble+y._1*nbHFiles.toDouble)/(nbFiles.toDouble),1) else ((y._1*nbSFiles.toDouble+x._1*nbHFiles.toDouble)/(nbFiles.toDouble) ,0)) .map(x => (x._1,x._2._1))
 	      //probaW -> class org.apache.spark.rdd.MapPartitionsRDD



              //Compute the probability P(occurs, class) for each word.

              val probaH = nbHFiles.toDouble / nbFiles.toDouble // the probability that an email belongs to the given class.
              //probaH -> double
              val probaS = nbSFiles.toDouble / nbFiles.toDouble
              //probaS -> double
              // Compute mutual information for each class and occurs
              val MITrueHam = computeMutualInformationFactor(probaHW, probaW, probaH, 0.2 / nbFiles) // the last is a default value
              //MITrueHam -> class org.apache.spark.rdd.MapPartitionsRDD
              val MITrueSpam = computeMutualInformationFactor(probaSW, probaW, probaS, 0.2 / nbFiles)
              //MITrueSpam -> class org.apache.spark.rdd.MapPartitionsRDD
              val MIFalseHam = computeMutualInformationFactor(probaHW.map(x => (x._1, 1.00001 - x._2)), probaW, probaH, 0.2 / nbFiles)
              //MIFalseHam -> class org.apache.spark.rdd.MapPartitionsRDD
              val MIFalseSpam = computeMutualInformationFactor(probaSW.map(x => (x._1, 1.00001 - x._2)), probaW, probaS, 0.2 / nbFiles)
              //MIFalseSpam -> class org.apache.spark.rdd.MapPartitionsRDD


              println("print top MIFalseSpam prob:")
              MIFalseSpam.top(10)(Ordering[Double].on(x => x._2)).foreach{ println }
              println("print top MIFalseHam prob:")
              MIFalseHam.top(10)(Ordering[Double].on(x => x._2)).foreach{ println }
              println("print top MITrueSpam prob:")
              MITrueSpam.top(10)(Ordering[Double].on(x => x._2)).foreach{ println }
              println("print top MITrueHam prob:")
              MITrueHam.top(10)(Ordering[Double].on(x => x._2)).foreach{ println }

              //sum the mutual information
              val MI :RDD[(String, Double)] = MITrueHam.union(MITrueSpam).union(MIFalseHam).union(MIFalseSpam).reduceByKey( (x, y) => x + y)
              //MI -> class org.apache.spark.rdd.ShuffledRDD
              //These words must be also stored on HDFS in the file “/tmp/topWords.txt”.
              val path: String = "/tmp/topWords.txt"
              //path -> class java.lang.String
              val topTenWords: Array[(String, Double)] = MI.top(20)(Ordering[Double].on(x => x._2))
              //topTenWords -> class [Lscala.Tuple2;
              //save the top 20 words
              sc.parallelize(topTenWords).keys.coalesce(1, true).saveAsTextFile(path)
      }
      else
              println("Please write te directory where the ham and spam")
}

\end{lstlisting}


%\section{Scala useful functions}
%\begin{itemize}
%	\item wholeTextFiles - lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with textFile, which would return one record per line in each file.
%	\item map(func) - return a new distributed dataset formed by passing each element of the source through a function func.  
%	\item flatMapValues -
%
%		rdd.flatMapValues(x => (x to 5))
%
%		It is applied on an rdd {(1,2),(3,4),(3,6)} and the output of the transformation is {(1,2),(1,3),(1,4),(1,5),(3,4),(3,5)}
%
%		faltMapValues works on each value associated with key. In above case {x to 5} means each value will be incremented till 5.
%
%		Taking first pair where you have {1,2} , here key is 1 and value is 2 so there after applying transformation it will become (1,2),(1,3),(1,4),(1,5).
%	\item mapValues - 
%		\begin{lstlisting}
%		val m = Map( ``a'' -> 2, ``b'' -> 3 )
%
%		// both
%
%		m.mapValues(_ * 5)
%		m.transform( (k,v) => v * 5 )
%		// deliver the same result.
%
%
%		\end{lstlisting}
%	\item filter(func) -  return a new dataset formed by selecting those elements of the source on which func returns true. 
%	\item case -
%		\begin{lstlisting}
%import scala.util.Random
%
%val x: Int = Random.nextInt(10)
%
%x match {
%  case 0 => "zero"
%  case 1 => "one"
%  case 2 => "two"
%  case _ => "many"
%}
%
%
%def matchTest(x: Int): String = x match {
%  case 1 => "one"
%  case 2 => "two"
%  case _ => "many"
%}
%matchTest(3)  // many
%matchTest(1)  // one
%
%
%		\end{lstlisting}
%	\item reduce - aggregate the elements of the dataset using a function func (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.  
%	\item reduceByKey - when called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.  
%	\item Ordering.by - Ordering is a trait whose instances each represent a strategy for sorting instances of a type. Ordering's companion object defines many implicit objects to deal with subtypes of AnyVal (e.g. Int, Double), String, and others. To sort instances by one or more member variables, you can take advantage of these built-in orderings using Ordering.by and Ordering.on:
%		\begin{lstlisting}[language=scala]
%import scala.util.Sorting
%val pairs = Array(( ``a'', 5, 2), (``c'', 3, 1), (``b'', 1, 3))
%
%// sort by 2nd element
%Sorting.quickSort(pairs)(Ordering.by[(String, Int, Int), Int](_._2))
%
%// sort by the 3rd element, then 1st
%Sorting.quickSort(pairs)(Ordering[(Int, String)].on[
%                        (String, Int, Int)](( _._3, _._1)))
%		\end{lstlisting}
%
%	\item fullOuterJoin - 
%	\item join - when called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin. 
%	\item leftOuterJoin -
%
%		\begin{figure}[!htb]
%			\includegraphics[width=10.5cm]{join-types.png}
%		\end{figure}
%
%	\item getOrElse -  
%	\item math.log - The object Math contains methods for performing basic numeric operations such as the elementary exponential, logarithm, square root, and trigonometric functions.  
%\begin{lstlisting}[language=scala]
%def log (x: Double): Double 
%\end{lstlisting}
%	\item toDouble -
%\begin{lstlisting}[language=scala]
%scala> "100".toDouble
%res1: Double = 100.0
%
%\end{lstlisting}
%	\item toSet -
%\begin{lstlisting}[language=scala]
%scala> val arr = Array("a", "b", "c")
%arr: Array[java.lang.String] = Array(a, b, c)
%
%scala> arr.toSet
%res1: scala.collection.immutable.Set[java.lang.String] = Set(a, b, c)
%\end{lstlisting}
%	\item takeOrdered - return the first n elements of the RDD using either their natural order or a custom comparator.
%	\item foreach - run a function func on each element of the dataset. This is usually done for side effects such as updating an accumulator variable (see below) or interacting with external storage systems. 
%	\item swap -
%		\begin{lstlisting}[language=scala]
%		scala> val pair = (1,2)
%		pair: (Int,Int) = (1,2)
%
%		scala> val swappedPair = pair.swap
%		swappedPair: (Int,Int) = (2,1)
%		\end{lstlisting}
%\end{itemize}
%%%% End document
\end{document}
